---
title: "Homework 2, Group G"
author: "Abdalghani, Demirbilek, Dorigo, Miolato"
date: "Spring 2020"
output:
  html_document:
    toc: yes
  beamer_presentation:
    highlight: tango
  include: null
  ioslides_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: tango
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
library(MASS)
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```


## Laboratory 2 
### Exercise 1

* Check the biased nature of $s_b^2$ via MC simulation, generating $n=10$ iid values from a normal distribution. Plot also $s^2$ and comment the difference.

*solution :*


```{r basic 1, echo=TRUE}
set.seed(123)
R <- 1000
n <- 10
sigma <- 1

samples <- matrix(NA,R,n)
samples_var <- c()
samples_stat <- array(0, c(1, 2, R))

#generate n values R times
for (i in 1:R){
  samples[i, ] <- rnorm(n, 0, 1)
  samples_var[i] <- sum((samples[i, ]-mean(samples[i, ]))^2/n)
} 
samples_stat[1, 1, ] <- apply(samples[ , ], 1 , mean )
samples_stat[1, 2, ] <- apply(samples[ , ], 1 , var )

par (mfrow=c(1,2), oma=c(0,0,0,0))
hist(samples_var, breaks= 40, probability = TRUE, 
xlab=expression(s_b^2), main= bquote(s_b^2), cex.main=1.5)
curve(((n)/sigma^2) * dchisq(x * ((n)/sigma^2), df = n - 1),
add = TRUE, col="blue", lwd=2, main="N(0,1)")

hist(samples_stat[1,2, ], breaks= 40, probability = TRUE, 
xlab=expression(s^2), main= bquote(s^2), cex.main=1.5)
curve(((n-1)/sigma^2) * dchisq(x * ((n-1)/sigma^2), df = n - 1),
add = TRUE, col="red", lwd=2, main="N(0,1)")

```



### Exercise 2

What happens if a great player decides to join you, now? Try to simulate the data and perform the test again.


By assuming he is a great player, and that his shots along the game will hit the highest points with great probability and the lowest point with low probability. We assign the following hitting probabilities:

-Zone 1 (from 1 to 3 points): p1=1/16;

-Zone 2 (from 4 to 6 points); p2=3/16;

-Zone 3 (from 7 to 9 points); p3=5/16;

-Zone 4 (the highest points in the middle of the target, say 10, 25, 50 points): p4=7/16;

*solution :*

```{r basic 2, echo=TRUE}
set.seed(101)
n <- 50
K <- 4
# generate the values
y <- sample( 1:K, n, replace=TRUE, prob =c( 1/16, 3/16, 5/16, 7/16))
observed <- table(y)
expected <- c( n*(1/16), n*(3/16), n*(5/16), n*(7/16))
x2 <- sum( (observed-expected)^(2)/expected)

#manually compute the p-value
pchisq(x2, df =K-1, lower.tail =FALSE )


chisq.test( observed, p = c( 7/16, 5/16, 3/16, 1/16)   )
```
```{r basic 3, echo=TRUE}
set.seed(101)
n <- 50
K <- 4
M <- 7

team <- matrix(0, M, n)
obs <- matrix(0, M, K)
exp <- matrix(0, M, K)

for (i in 1:M) {
  if(i == 7){
    team[i, ] <- sample(1:K, n, replace=TRUE, prob =c( 1/16, 3/16, 5/16, 7/16))
    obs[i, ] <- table(team[i,])
    exp[i, ] <- c( n*(1/16), n*(3/16), n*(5/16), n*(7/16) )
  }
  
  team[i, ] <- sample(1:K, n, replace=TRUE, prob =c( 7/16, 5/16, 3/16, 1/16))
  obs[i, ] <- table(team[i, ])
  exp[i, ] <- c( n*(7/16), n*(5/16), n*(3/16), n*(1/16) )
}

chisq.test( obs, p = c( 7/16, 5/16, 3/16, 1/16)   )



diff <- matrix(0, M, K)
for (j in (1:M))
{
  for (i in (1:K))
  {
    diff[j, i] = ((obs[j, i] - exp[j, i])^2/exp[j, i])
  }
}

# manually compute the p-value
x2 <- sum(diff)
pchisq(x2, df = K-1, lower.tail =FALSE )

```

### Exercise 3

Are the Instagram and Twitter account somehow associated? Perform a correlation test, compute the p-value and give an answer. Here is the dataframe.

```{r basic 4, echo=TRUE}

Owners <- c( "Katy Perry", "Justin Bieber", "Taylor Swift", "Cristiano Ronaldo",
                   "Kim Kardashian", "Ariana Grande", "Selena Gomez", "Demi Lovato")
Instagram <- c( 69, 98,107, 123, 110, 118, 135, 67)
Twitter <- c( 109, 106, 86, 72, 59, 57, 56, 56)
plot( Instagram, Twitter, pch=21, bg=2, xlim=c(60, 150), ylim=c(40, 120) )
text( Instagram[-6], Twitter[-6]+5, Owners[-6], cex=0.8 )
text( Instagram[6], Twitter[6]-5, Owners[6], cex=0.8 )

      
```

*solution :*


```{r basic 5, echo=TRUE}

Owners <- c( "Katy Perry", "Justin Bieber", "Taylor Swift", "Cristiano Ronaldo",
                   "Kim Kardashian", "Ariana Grande", "Selena Gomez", "Demi Lovato")
Instagram <- c( 69, 98,107, 123, 110, 118, 135, 67)
Twitter <- c( 109, 106, 86, 72, 59, 57, 56, 56)


cor(Instagram, Twitter, method = c("pearson"))
#cor : computes the correlation coefficient
cor.test(Instagram, Twitter, method=c("pearson"))
#cor.test : computes the correlation between x, y and returns correlation coefficient and Significance level(or p-value)
      
```



### Exercise 4

Compute analitically $ J(\gamma,\gamma;y),J(\gamma,\beta;y),J(\beta,\beta;y)$.

*solution :*


* $J(\gamma,\gamma;y) = \frac{\partial^2 l(\gamma,\beta;y)}{\partial\gamma^2}$
$  = \frac{n}{\gamma^2} + \sum\limits_{i=1}^n (\frac{y_i}{\beta})^2 [log(\frac{y_i}{\beta})^2]$

* $J(\gamma,\beta;y) = \frac{\partial^2 l(\gamma,\beta;y)}{\partial\gamma\beta} $
$  = \frac{n}{\beta} - \frac{1}{\beta} \sum\limits_{i=1}^n (\frac{y_i}{\beta})^\gamma log(\frac{y_i}{\beta})^2$

* $J(\beta,\beta;y) = \frac{\partial^2 l(\gamma,\beta;y)}{\partial\beta^2} $
$  = \frac{n*\gamma}{\beta^2} - \frac{\gamma(\gamma+1)}{\beta^2} \sum\limits_{i=1}^n (\frac{y_i}{\beta})^\gamma$



### Exercise 5 

Produce the contour plot for the **quadratic approximation** of the log-likelihood, based on the Taylor series:

$l(\theta)−l(\hat\theta) \approx −\frac{1}{2}(\theta−\hat\theta)^T J(\hat\theta)(\theta−\hat\theta)$.


*solution :*


```{r basic 6, echo=TRUE}
log_lik_weibull_Qapprox <- function(theta){
    
}

y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
 170.3, 148, 140, 118, 144, 97)
n <- length(y)

 #define parameters grid
 gamma <- seq(0.1, 15, length=100)
 beta <- seq(100,200, length=100)
 parvalues <- expand.grid(gamma,beta)
 
 llikvalues <- apply(parvalues, 1, log_lik_weibull_Qapprox)
 #llikvalues <- matrix(-llikvalues, nrow=length(gamma), ncol=length(beta), byrow=F)
#conf.levels <- c(0,0.5,0.75,0.9,0.95,0.99)
 
      
```

... Not completed








## Core Statistics

### Exercise 3.3

Rewrite the following, replacing the loop with efficient code:
```{r basic 7, echo=TRUE}
n <- 100000; z <- rnorm(n)
zneg <- 0;j <- 1
for (i in 1:n) {
  if (z[i]<0) {
    zneg[j] <- z[i]
    j <- j + 1
  }
}

```
Confirm that your rewrite is faster but gives the same result.

*solution :* 

```{r basic 8, echo=TRUE}
set.seed(101)
n <- 100000; z <- rnorm(n)
zneg <- 0;j <- 1

start1 <- Sys.time()
for (i in 1:n) {
  if (z[i]<0) {
    zneg[j] <- z[i]
    j <- j + 1
  }
}
end1 <- Sys.time()
Naive_time <- end1 - start1


zneg_2 <- 0;
start2 <- Sys.time()
zneg_2 <- z[ z < 0 ]
end2 <- Sys.time()

adv_time <- end2 - start2 

print(cat("Naive function took : " , Naive_time))
print(cat("Alternative function took : " , adv_time))

```



### Exercise 3.5
Consider solving the matrix equation $Ax = y$ for $x$, where $y$ is a known n vector and $A$ is a known $n×n$ matrix. The formal solution to the problem is $x = A^{−1}y$, but it is possible to solve the equation directly, without actually forming $A^{−1}$. This question explores this direct solution. Read the help file for $solve$ before trying it.


a. First create an $A$, $x$ and $y$ satisfying $Ax = y$.

*solution :*
```{r basic 9, echo=TRUE}
set.seed(0); 
n <- 1000
A <- matrix(runif(n*n),n,n); 
x.true <- runif(n)
y <- A%*%x.true

```
The idea is to experiment with solving $Ax = y$ for $x$, but with a known truth to compare the answer to.



b. Using solve, form the matrix $A^{−1}$ explicitly and then form $x_1 = A^{−1}y$. Note how long this takes. Also assess the mean absolute difference between $x1$ and $x.true$ (the approximate mean absolute ‘error’ in the solution).

*solution :*

```{r basic 10, echo=TRUE}
start1 <- Sys.time()
A_inv <- solve(A)
x1 <- A_inv%*%y
end1 <- Sys.time()

naive_time = end1-start1
naive_time

mean_abs1 <- mean(abs(x1 - x.true))
mean_abs1
```

c. Now use solve to directly solve for $x$ without forming $A^{−1}$. Note how long this takes and assess the mean absolute error of the result.

*solution :*

```{r basic 11, echo=TRUE}
start2 <- Sys.time()
x2 <- solve(A,y)
end2 <- Sys.time()

opt_time = end2-start2
opt_time

mean_abs2 <- mean(abs(x2 - x.true))
mean_abs2
```

d. What do you conclude?

*solution :*

By assing the taken time and the mean absolute difference between both solutions, it is given that the average absolute difference using $solve$ function directly is much lesser than using $solve$ with inverse matrix. Also, it takes much less time. 



## Data Analysis and Graphics Using R

### CH3.Exercise 11
The following data represent the total number of aberrant crypt foci (abnormal growths in the colon) observed in seven rats that had been administered a single dose of the carcinogen azoxymethane and sacrificed after six weeks (thanks to RanjanaBird, Faculty of Human Ecology, University of Manitoba for the use of these data):
87 53 72 90 78 85 83
Enter these data and compute their sample mean and variance. Is the Poisson model appropriate for these data? To investigate how the sample variance and sample mean differ under the Poisson assumption, repeat the following simulation experiment several times:
x <- rpois(7, 78.3)
mean(x); var(x)

*solution :*

```{r basic 12, echo=TRUE}
data <- c(87, 53, 72, 90, 78, 85, 83)
smean = mean(data)
svar = var(data)
smean ; svar

#the variance is almost the double of the mean, may Poisson is not appropriate

times = 100
samples_stat <- as.data.frame(matrix(NA,2,times))
colnames(samples_stat) = c("mean", "variance")

# n = 7; #lambda = 78.3
for (i in 1:times){
  x <- rpois(7, 78.3)
  samples_stat[i,"mean"] = mean(x)
  samples_stat[i,"variance"] = var(x)
}


plot(samples_stat$variance, samples_stat$mean, ylab = expression(mean), xlab = expression(variance), main = "Poisson Samples mean and varaince") + abline(h = smean , col = "red") + abline(v = svar, col = "blue")

```


### CH3.Exercise 13

A Markov chain for the weather in a particular season of the year has the transition matrix, from
one day to the next:

$Pb = \begin{bmatrix} & Sun & Cloud & Rain \\ Sun & 0.6 & 0.2 & 0.2 \\ Cloud & 0.2 & 0.4 & 0.4 \\ Rain & 0.4 & 0.3 & 0.3 \end{bmatrix}$

It can be shown, using linear algebra, that in the long run this Markov chain will visit the states according to the stationary distribution:
 Sun  Cloud Rain
0.641 0.208 0.151

A result called the ergodic theorem allows us to estimate this distribution by simulating the Markov chain for a long enough time.

(a) Simulate 1000 values, and calculate the proportion of times the chain visits each of the states. Compare the proportions given by the simulation with the above theoretical proportions.

*solution :*

```{r basic 13, echo=TRUE}

Pb <- matrix(c(0.6, 0.2, 0.2, 
                     0.2, 0.4, 0.4,
                     0.4, 0.3, 0.3), nrow = 3, ncol = 3, byrow = TRUE)

Markov = function(R, init, Mat ){
  chain = numeric(R)
  chain[1] = init+1
  
  for(i in 2:R){
    chain[i] = sample(x = 1:3, size =1, prob = Mat[chain[i-1], ])
  }
  chain - 1
  
}

results <- table(Markov(R= 1000, init = 0, Mat= Pb))
results

print(results/1000)

```


(b) Here is code that calculates rolling averages of the proportions over a number of simulations and plots the result. It uses the function rollmean() from the zoo package.

```{r basic 14, echo=TRUE}
library(zoo)
library(lattice)
plotmarkov <- function(n=10000, start=0, window=100, transition=Pb, npanels=5){
      xc2 <- Markov(n, start, transition)
      mav0 <- rollmean(as.integer(xc2==0), window)
      mav1 <- rollmean(as.integer(xc2==0), window)
      npanel <- cut(1:length(mav0), breaks=seq(from=1, to=length(mav0),
      length=npanels+1), include.lowest=TRUE)
      df <- data.frame(av0=mav0, av1=mav1, x=1:length(mav0),
      gp=npanel)
      print(xyplot(av0+av1 ~ x | gp, data=df, layout=c(1,npanels),
      type="l", par.strip.text=list(cex=0.65),
      scales=list(x=list(relation="free"))))
}
plotmarkov(window = 10)
plotmarkov(window = 100)
plotmarkov(window = 1000)
```

Try varying the number of simulations and the width of the window. How wide a window is needed to get a good sense of the stationary distribution? This series settles down rather quickly to its stationary distribution (it “burns in” quite quickly). A reasonable width of window is, however, needed to give an accurate indication of the stationary distribution.

### CH4.Exercise 6
Here we generate random normal numbers with a sequential dependence structure:

```{r basic 15, echo=TRUE}
y1 <- rnorm(51)
y <- y1[-1] + y1[-51]
par (mfrow=c(1,2))
acf(y1) # acf is ‘autocorrelation function
acf(y)
```

Repeat this several times. There should be no consistent pattern in the acf plot for different random samples y1. There will be a fairly consistent pattern in the acf plot for y, a result of the correlation that is introduced by adding to each value the next value in the sequence.
 

### CH4.Exercise 7
Create a function that does the calculations in the first two lines of the previous exercise.
Put the calculation in a loop that repeats 25 times. Calculate the mean and variance for each vector y that is returned. Store the 25 means in the vector av, and store the 25 variances in the vector v. Calculate the variance of av.

*solution :*

```{r basic 16, echo=TRUE}
auto_cor <- function(){
  y1 <- rnorm(51) # this is iid
  y <- y1[-1] + y1[-51] #this is not iid
  y
}
av <- numeric(25)
v <- numeric(25)
for (j in 1:25){
  y <- auto_cor()
  av[j] <- mean(y)
  v[j] <- var(y)
}

var(av)
```




















